bert:
  # For bert models, path from hugging face transformers library
  model_path: 'SpanBERT/spanbert-base-cased'
  chunk_len: 128
  stride: 0
  token_embed_dim: 768
elmo:
  # possible_options for model_size: small, medium, original; for more information: https://allenai.org/allennlp/software/elmo
  # now elmo not available in Russia
  model_size: 'small'
  num_output_representations: 1
  dropout: 0.4
  requires_grad: False
  device: None
untrained:
  filter_widths: [3, 4, 5]
  filter_size: 50
  char_embedding_size: 8
  lstm_hidden: 200
  lstm_dropout_rate: 0.4
  lexical_dropout_rate: 0.5
  num_lstm_layers: 2

# Token embedding model, possible options: elmo, bert, untrained
embedding_model: 'bert'
max_segments: 11
max_top_antecedents: 50
max_training_parts: 50
top_span_ratio: 0.4


ffnn_size: 150
dropout_rate: 0.2
max_span_len: 30
learning_rate: 0.001
lr_decay_gamma: 0.1
lr_decay_step: 100
num_epoch: 150

savepath: './model'


